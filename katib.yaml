name: Tune Hyperparameters
description: Generic hyperparameter tuning using Katib for a given model type.
inputs:
  - name: model_type
    type: String
    description: "Model type: Choose one of ['sklearn', 'xgboost', 'pytorch']"
  - {name: X_train, type: JsonArray}
  - {name: y_train, type: JsonArray}
outputs:
  - {name: best_hyperparams, type: JsonArray}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn' 'kubernetes' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn' 'kubernetes' --user) && "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import time
        from kubernetes import client, config

        parser = argparse.ArgumentParser()
        parser.add_argument("--model_type", type=str, required=True)
        parser.add_argument("--X_train", type=str, required=True)
        parser.add_argument("--y_train", type=str, required=True)
        parser.add_argument("--best_hyperparams", type=str, required=True)
        args = parser.parse_args()

        # Load dummy training data (not used here but accepted for pipeline compliance)
        with open(args.X_train, "r") as f:
            X_train = json.load(f)
        with open(args.y_train, "r") as f:
            y_train = json.load(f)

        config.load_incluster_config()

        model_type = args.model_type.lower()
        name = f"{model_type}-katib-exp"
        namespace = "kubeflow"

        search_spaces = {
            "sklearn": [
                {"name": "max_depth", "parameterType": "int", "feasibleSpace": {"min": "3", "max": "10"}},
                {"name": "n_estimators", "parameterType": "int", "feasibleSpace": {"min": "50", "max": "150"}}
            ],
            "xgboost": [
                {"name": "max_depth", "parameterType": "int", "feasibleSpace": {"min": "3", "max": "12"}},
                {"name": "learning_rate", "parameterType": "double", "feasibleSpace": {"min": "0.01", "max": "0.3"}}
            ],
            "pytorch": [
                {"name": "lr", "parameterType": "double", "feasibleSpace": {"min": "0.0001", "max": "0.1"}},
                {"name": "batch_size", "parameterType": "int", "feasibleSpace": {"min": "16", "max": "128"}}
            ]
        }

        if model_type not in search_spaces:
            raise ValueError(f"Unsupported model type: {model_type}")

        parameters = search_spaces[model_type]
        metric_name = "accuracy"
        maximize = True

        # Trial container logic - dummy task, replace with real image & logic
        experiment = {
            "apiVersion": "kubeflow.org/v1beta1",
            "kind": "Experiment",
            "metadata": {"name": name, "namespace": namespace},
            "spec": {
                "objective": {
                    "type": "maximize" if maximize else "minimize",
                    "goal": 0.95,
                    "objectiveMetricName": metric_name
                },
                "algorithm": {"algorithmName": "random"},
                "parameters": parameters,
                "trialTemplate": {
                    "primaryContainerName": "training-container",
                    "trialParameters": [
                        {"name": p["name"], "description": "", "reference": p["name"]}
                        for p in parameters
                    ],
                    "trialSpec": {
                        "apiVersion": "v1",
                        "kind": "Pod",
                        "spec": {
                            "containers": [
                                {
                                    "name": "training-container",
                                    "image": "python:3.9",
                                    "command": ["python3", "-c"],
                                    "args": [
                                        "from sklearn.datasets import load_iris; "
                                        "from sklearn.ensemble import RandomForestClassifier; "
                                        "from sklearn.model_selection import cross_val_score; "
                                        f"X, y = load_iris(return_X_y=True); "
                                        f"clf = RandomForestClassifier("
                                        + ", ".join([
                                            f"{p['name']}=float('${{trialParameters.{p['name']}}}')"
                                            if p["parameterType"] == "double"
                                            else f"{p['name']}=int('${{trialParameters.{p['name']}}}')"
                                            for p in parameters
                                        ])
                                        + "); "
                                        "acc = cross_val_score(clf, X, y, cv=3).mean(); "
                                        "print('accuracy:', acc)"
                                    ]
                                }
                            ],
                            "restartPolicy": "Never"
                        }
                    }
                }
            }
        }

        # Submit experiment
        k8s_client = client.CustomObjectsApi()
        k8s_client.create_namespaced_custom_object(
            group="kubeflow.org",
            version="v1beta1",
            namespace=namespace,
            plural="experiments",
            body=experiment
        )

        print("Waiting for experiment to complete...")
        best_trial = None
        for _ in range(100):
            status = k8s_client.get_namespaced_custom_object_status(
                group="kubeflow.org",
                version="v1beta1",
                name=name,
                namespace=namespace,
                plural="experiments",
            )
            conds = status.get("status", {}).get("conditions", [])
            if any(c["type"] == "Succeeded" and c["status"] == "True" for c in conds):
                best_trial = status["status"]["currentOptimalTrial"]["parameterAssignments"]
                break
            time.sleep(10)

        if not best_trial:
            best_trial = [{"name": p["name"], "value": "default"} for p in parameters]

        # Save the best hyperparameters
        os.makedirs(os.path.dirname(args.best_hyperparams), exist_ok=True)
        with open(args.best_hyperparams, "w") as f:
            json.dump({p["name"]: float(p["value"]) if "." in p["value"] else int(p["value"]) for p in best_trial}, f)
    args:
      - --model_type
      - {inputValue: model_type}
      - --X_train
      - {inputPath: X_train}
      - --y_train
      - {inputPath: y_train}
      - --best_hyperparams
      - {outputPath: best_hyperparams}
